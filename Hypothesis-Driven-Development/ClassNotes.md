# HYPOTHESIS-DRIVEN DEVELOPMENT – CLASS NOTES

## Week 1 - How do we know if we're building for a user that doesn't exist?
"_How do you go from backlog grooming to blockbuster results with agile? Hypothesis-driven decisions. Specifically, you need to shift your teammates focus from their natural tendency to focus on their own output to focusing out user outcomes. Easier said than done, but getting everyone excited about results of an experiment is one of the most reliable ways to get there. This week, we’ll focus on how you get started in a practical way._"

### Week 1: Introduction

__The Persona Hypothesis__    
0:00 - 0:08
- _Before we dive into each of the individual hypothesis areas, I thought we might loop through a few overviews._
- Overview/Recap Persona - a humanized, but specific view of who the user is, and what makes them tick.    


__The Problem/JTBD Hypothesis__    
0:01 - 0:06
- _Let's talk about the problem or job to be done hypothesis._
- Overview/Recap Problem / Jobs to be Done (JTBD) - an underlying need of the persona - i.e., durable problem, job, habit, desire that exists out there that we've observed. It is de-coupled from our proposition (proposed solution) because the problem/JBTD is extremely durable but the solution may change. The JTBD is found by interviews and his done in conjunction with the interviews for defining the persona.    


__The Demand/Value Hypothesis__    
0:00 - 0:09
- _Well, let's talk about this value or demand hypothesis, and how we go through and how we ask and answer these questions._
- Value or demand hypothesis - a hypothesis that a proposition to the current alternative or workaround will result in a more favorable or valuable outcome. The Value Hypothesis is easy to draft if you have a well-researched persona and problem/JTBD hypothesis. It may seem obvious, but it will help the team focus. In the context of Lean Startup, an MVP (minimally viable product) is developed as a product proxy to avoid wasted development and poor launches. It is not version 1.0 of the solution or product.    


__The Usability Hypothesis__    
0:00 - 0:03
- _Let's talk about the usability hypothesis,_
- Usability Hypothesis - Agile user stories that are informed by the persona, problem/JTBD, and value hypotheses. Method: storyboard how the intended persona/user will use your product. Develop parallel prototypes and test each to see which one is more usable by the targeted persona.


__The Functional Hypothesis__    
0:00 - 0:15
- _As we look at this next hypothesis area, we're going to move from this domain of continuous design over to this domain of continuous delivery. We're going to look at this functional hypothesis and how we do this work._
- Functional Hypothesis - takes the form of, Given [the user is in some context], When [the user indicates some preference or desire] And [the user commits, takes an action or makes a request] Then [a result occurs]



### Week 1: Identifying & Testing Persona & Problem Hypothesis

__Example Personas and Problem Scenarios__    
0:38 - 0:48
- _what we're trying to do with these personas is tell a story, humanize this customer or this user for yourself and your team and really think about who they are and what makes them tick._


__Setting Up Interviews__    
2:18 - 2:20
- _Finally, what about the interviews themselves?_
- Logistics to conducting interviews - rated most to least favorable 1. In person - in situ 2. In person - ex situ 3. Video call 4. Phone call


__Prepping for Subject Interviews__    
0:00 - 0:14
- _Let's say you're ready to invest in investigating your persona, your job to be done hypotheses. You're going to go out and talk to some subjects. How do you get ready? Think about how you're going to do this. Well, here's how I like to do it in general._
- For a new persona, 1. Draft Persona 2. Draft Interview Guide (not a questionnaire, rather a guide that goes from general to specific) 3. Conduct interviews 4. Revise Persona and Problem/JTBD - as you continue to conduct interviews 5. Test - test whether the interviews are answering the questions that you set out to explore.


__Conducting the Interview__  
0:14 - 0:17
- _Here's a checklist of things that you want to make sure of._
- Checklist:
1. Screener question. Do you have a screener to confirm that you are dealing with a person who fits your persona? 
2. Interview guide. Do you have an interview guide? 
3. Intro script. Do you have an introduction script that describes how you're interested in what they do? Do not introduce what you plan to build. Do not assume that they are familiar with being an interview subject. You can tell them generally what you're up to, but don't give them specifics. Instead, tell them that you'd like to hear about who they are, how they do things, or what their concerns may be. 
4. Paperwork. Do you have any required releases? How will you take notes during the interview? 
5. Day in the Life. Do you intend to follow up with them in a Day in the life exercise?


__Conducting the Interview__  
4:36 - 5:02
- _And what we really want to do there is anchor in a specific example. And this is hard, because what they'll usually tell you paradoxically is like, that last example isn't really typical. But really they're all atypical. They think you want some averaged out generic answer, and that is the opposite of what you want. So, you're going to tell them, hey, can you tell me about the last time? That wasn't typical. You just tell them, that's okay, just tell me about the last time.__
- When interviewing to uncover the interview subject's experience, in reality, all answers rooted in actual experience are atypical. Make sure they know that's perfectly ok so that they give you the most valuable feedback possible.



### Week 1: Intro to Design Sprints

__You and Your Next Design Sprint__
0:00 - 0:09
- _Design sprints are a way that teams make time to do the work of continuous design in their Agile cadences._
- Design sprints are time boxed (1-5 days), structured, and focused on answering a design question, and then moving on. They can take different formats: e.g., interviews for persona and problem hypothesis, run and test an MVP (minimally viable product), test usability.   
___Resource:___ Tutorial on Alex Cowan's Site: Venture Design Sprints. This page provides an overview of design sprints, which are the primary topic of Course 2 (Running Product Design Sprints).  
<(https://user-images.githubusercontent.com/81437067/154092109-e9532a8e-daf5-4900-8c8a-e7da193de738.png)>


__The Practice of Time Boxing__   
0:00 - 0:08
- _I thought we'd talk a little bit more about this practice of timeboxing because it may have applications for you beyond just the design sprints._
- In a design sprint, you may want to time box individual activities - for example drafting a persona or a user story. shorten exercises from 60-90 minutes to 30 minutes. It will cause participants to be more engaged more quickly. It will support time for "show and tell". Also, if someone puts in a lot of work on an individual task, they can become overly attached to that work and resistant to feedback and iteration. Avoid this by providing just enough time for a solid attempt at the task.


__Overview of the Persona and Problem Sprint__    
1:50 - 2:03
- _On the fifth day, the basic idea is that, you're cleaning up your work, you're compiling your transcripts, your making sure that those expensive assets that you invested all this time in creating are something you can refer back to._
- Remember to keep transcripts. Things that may not seem relevant while you're out conducting interviews may prove to be very important later on. Make sure to keep detailed transcripts as you work through the interview process.


__How Do I Sell the Idea of a Design Sprint__   
0:01 - 0:04
- _How do you get permission to do this? How do you get everybody bought in?_
1. Don't make a big deal about it - i.e., don't say that we are making a major change in what we and how we do it. 
2. Think about about what you stakeholders care about and are motivated by - may be excited to get a couple days out of the office and into a change of routine. 
3. Make sure you get buy-in if you plan to observe staff. Tell them you are there to learn about how they do things right now, so that you can do your work for them better. 
4. Re-name it if "design sprint" isn't getting the right reaction - try calling it a lean research sprint. 
5. Don't be afraid to cite the success rates on prior products. Remind participants the problems with the ways you've done things before, and be able to explain why this alternative is going to be better. 
6. Improvise if you don't get buy-in, and incorporate it into daily activities/stand-ups.



## Week 2 - How do we reduce waste & create wins by testing our propopsitions before we build them?
"_Nothing will help a team deliver better outcomes like making sure they’re building something the user values. This might sound simple or obvious, but I think after this week it’s likely you’ll find opportunities to help improve your team’s focus by testing ideas more definitively before you invest in developing software. In this module, you’ll learn how to make concept testing an integral part of your product pipeline. We’ll continue to apply methods from Lean Startup, looking at how they pair with agile. We’ll look at how high-functioning teams design and run situation-appropriate experiments to test ideas, and how that works before the fact (when you’re testing an idea) and after the fact (when you’re testing the value of software you’ve released)._"

### Week 2: Identifying & Testing a Demand Hypothesis

__Creating More Wins__    
0:01 - 0:06
- _This week, we're going to look at how you define a value hypothesis._
- A Demand/Value Hypothesis should include: 
  - Problem Scenario about which people are. actually concerned, 
  - A Clear Alternative that is currently used and learned through interviews or work environment observation, 
  - A Value Proposition identifying something better than the alternative.


__Creating More Wins__    
3:24 - 3:35
- _Lean Startup is a domain specific extension of Agile where we're looking to test for value using MVPs, which are product proxies._
- Lean Startup - an extension of Agile where minimally viable products are created and used, as full product proxies, to test for value -- i.e, whether the MVP is worth developing further because it better enough than the currently used alternatives. The MVP can be quickly developed and tested with a smaller investment of time and resources compared to the investment in a more realized product. 
- A Demand/Value Hypothesis should include: 
  - Problem Scenario about which people are actually concerned, 
  - A Clear Alternative that is currently used and learned through interviews or work environment observation, 
  - A Value Proposition identifying something better than the alternative.


__Lean Startup and the Systematic Drive to Value__    
0:33 - 0:38
- _If you're familiar with Lean Startup, you probably seen this build measure learn process._
- Start with learn, Then build something - ideally not a product, Then measure the results. Work towards is a situation where you test the value hypothesis to get a definitive true or false.


__Lean Startup and the Systematic Drive to Value__    
0:59 - 1:08
- _I like to use this old standby the scientific method to kind of unpack this process a little further and a little more specifically for our purpose._
1. Idea: Start with an idea: validate persona and problem hypothesis. 
2. Hypothesis: Develop a value hypothesis that is declared in a way that it can be tested with either a true or false determination. 
3. Experimental Design: develop an MVP requiring a minimum of time and effort. 
4. Experimentation: drive toward a definitive moment where you can say true or false. 
5. Pivot or persevere: If false, pivot. If true, persevere and invest further in the MVP and see whether it is a "winner".


__Lean Startup and the Systematic Drive to Value__    
3:58 - 4:21
- _the questions of motivation or demand and usability they're related but they're not the same thing. And our job with Lean Startup and testing our value or demand hypothesis is to, first of all, declare a nice clear hypothesis where we pair away as many questions of usability so that we can test this independently._
- In testing the value hypothesis with a MVP, we strip away as many questions on usability as possible, so that we are focused on the value proposition.


__Focusing Your Ideas__   
0:15 - 0:42
- _one thing I find is usually helpful is having teams storyboard the process of how does a perspective user, persona, who you're going to deliver this value hypothesis to against these jobs to be done. How do they go from, not even knowing that your proposition or product exists to happy user where everything you want to have help him with the customer relationship has happened? What has to happen in between those things?_
- The storyboard should walk through the following stages of engagement: 
  - Attention: how do we get our target persona's attention directed toward our product? 
  - Interest: How do get our persona further engaged by being interested in our product? 
  - Desire: This will come from knowing our persona and the tension between their jobs to be done and their current alternative(s). 
  - Action: What is the next step that the persona must take before using the product. 
  - Onboarding: getting the customer onto our proposition, and using it successfully. 
  - Retention: The user experiences some initial reward and decides that they will continue to use the product.


__Focusing Your Ideas__   
4:56 - 5:02
- _there are these three principal engines of growth and it's good to think about which one is your principal engine_
- There are three engines of growth that you should think about and document in your final storyboard panel for retention: 
1. Paid: transactional. Persona/user pays for product. 
2. Viral: word of mouth. Persona/user tells others about their great experience with the product. 
Sticky: Customers come back to the product because they had a great experience.


__Focusing Your Ideas__   
6:17 - 6:27
- _this is a great way to kind of think through the whole user journey and unpack your assumptions into more soluble more testable pieces._


__Creating Assumptions__    
0:01 - 0:07
_ _storyboarding is a great way to get your assumptions articulated and really think through the whole user experience._
- Go through each of your storyboard panels, and make the implicit assumptions about what is happening explicit enough that they can be tested. These tests will be through MVP sub-components that might involve a rapid prototype, a concierge service, or other things that act as a temporary stand-in for your ideal product.


__Learning What's Valuable__    
4:26 - 4:29
- _The smoke test, is basically seeing if you can sell some. This is really good at getting a definitive result about whether you can bring customers into your conversion funnel to sell them or get them using your product. The depth of observation is low, but the definition of the result that you're going to get is very high._


__Learning What's Valuable__    
0:00 - 0:11
-_Now that we've had some practice articulating our hypotheses or assumptions, let's have a look at how we can pair them with various test vehicles or MVPs._
- MVP Archetypes are not meant to be efficient or production-ready. Rather, they serve as a learning vehicle and give us direction on how/whether we want to proceed in developing a product. It's fine if the scenario illustrated by your MVP isn't completely realistic! - Three common service archetypes: 
  -	Concierge: hand create the user experience. Depth (amount of observation and learning gained) is high. Definition (how clearly you will get a conclusive negative or positive result) is low. 
  -	Wizard of Oz: show or fake a customer interaction. 
  -	Smoke Test. See if you can sell product.


__Learning What's Valuable__    
2:48 - 3:14
- _Concierge vehicle is a great early stage MVP if you have a new concept. It's depth, in other words, the amount of observation and learning you're going to get is really high. You're going to be there, hand creating the experience. You get a lot of observations which can be really useful. The definition, in other words, how clearly is it going to give you a positive or a negative conclusive result on your value hypothesis, is pretty low._
- Concierge: hand create the user experience. 
  -	Depth (amount of observation and learning gained) is high. 
  -	Definition (how clearly you will get a conclusive negative or positive result) is low.


__Learning What's Valuable__    
3:22 - 3:27
- _The Wizard of Oz is where you show or fake a customer interaction._
- The Wizard of Oz looks like a real experience to the persona, but on the backend, were faking it. 
  -	It is medium on depth, because generally you're just observing a small slice of the overall process. 
  -	It is medium on definition because you are testing motivation and usability at the same time, which is okay, but it's not as conclusive about giving you positive or negative on your value hypothesis.



### Week 2: What's it like to apply Lean Startup with Agile?

__Interview: Tristan Kromer on the Practice of Lean Startup__   
0:15 - 0:19
- _And what's hard about doing Lean Startup in the real world?_
- Lean Startup Challenges: 
1. Have to be supremely confident in yourself while also willing to be so skeptical of your assumptions that you need to test. 
2. The team has to reach the mindset before implementing Lean Startup, that you are trying to discover a new business model, and that everyone agrees that moving forward only happens when you produce knowledge. Everything else that doesn't produce knowledge may be necessary, but it doesn't move you forward (waste?). 
3. Choosing a practice and making it effective. There are many practices e.g., Scrum, Kanban, Business Model Canvas, Lean Model Canvas. Don't be dogmatic, choose one, have a goal, build a habit, promote a culture of experimentation.


__Interview: David Bland on the Practice of Lean Startup__    
0:09 - 0:22
- _David, when we've talked in the past, you mentioned that there's a certain kind of personality that finds it most easy and most ready to stick to this practice of lean startup and make it work, can you talk a little bit about that personality?_
-	Individual personality traits that support Lean Startup: "Strong beliefs held loosely" - meaning that you should have an opinion and/or vision, but you are open to being influenced by data. 
-	Company traits that support Lean Startup: 
1.	Cross functional teams - Design, product and engineering reside in the same team. Matrix organizations have difficulty breaking through functional silos to collaborate in a meaningful way. 
2.	Dedication and focus - teams try to focus on one idea. More than one idea or direction limits attention span, and inhibits speed. 
3.	Funding model is based on 12 weeks rather than full year - annual budgets don't support a discovery mindset, but rather commit teams to things that have been green-lighted to build. 
-	Leadership Culture that supports Lean Startup: Lead with questions. Have a goal but don't presume you have the answers and set the direction. That mindset inhibits experimentation and energy.


__Interview: Tristan Kromer on Creating a Culture of Experimentation Part 1__   
0:12 - 0:37
- _Let's talk about creating an environment that's lean startup-friendly, we'll say. The idea of information radiators or an informative workspace is a popular topic in Agile, things like Kanban boards and user story maps are popular. What would you put on the walls in the workspace to help facilitate experimentation through lean startup?_
- Information to post in front of your team: 
  -	Target customer persona. Display this avatar so that you always think from their perspective 
  -	Business model canvas: who is the customer, what is the value proposition, who are the partners? 
  -	The current week's focus: something that focuses on the primary risk that you are trying to eliminate that week - i.e., some assumption about your customers that you are trying to validate. What experiment is being run.


__Interview: Tristan Kromer on Creating a Culture of Experimentation Part 1__   
6:03 - 6:07
- _So write down your experiments. You must._
- Set up experiments in a rigorous fashion by writing it down and displaying it: 
  -	What is our hypothesis? 
  -	What is our fail condition?
  -	What is our time-box? 
  -	Schedule ahead the debrief of the experiments and the retrospective. If you do not have this rigor, then it's very likely you'll fall into some confirmation bias or hindsight bias trap.


__Interview: Tristan Kromer on Creating a Culture of Experimentation Part 2__   
2:38 - 3:17
- _Let's say I'm the product owner, I'm on an agile team, and I'm in a big company, and nobody's told us don't do Lean Startup. But nobody's told us we must all do Lean Startup unless we have to do. But I want to do it, and I'm on a self-organizing team. So pretty much, I can suggest things and kind of pitch them to the team, and we'll agree or not agree to try them out. What would you advise that person to do in terms of kind of pitching a minimum viable version of doing Lean Startup if you will, and then focusing on what's going to get the earliest forwards to show everybody how it can help them?_


__Interview: David Bland on Creating a Culture of Experimentation: Part 1__   
0:10 - 0:23
- _what is it like actually creating an environment where these evidence-based innovation practices flourish? What are some of the everyday practices that you think are most effective and most durable?_
- Necessary ceremonies and guidelines to maintain a culture of experimentation in an Agile fashion: 
- Necessary Ceremonies for Lean Startup: 
  - Daily Standup: What are you trying to learn today? How can we help each other? 
  - Weekly Planning Meeting: Assume one experiment per week 
  - Weekly retrospective: What was this week's experiment and what did you learn? 
  - Pivot, Persevere, or Kill Meeting: At least every 12 weeks with decision making stakeholders. Review the experiments conducted, and their results, with a recommendation. 
    - Pivot - to a different customer, problem, or solution. 
    - Persevere - on the right track and need more data. 
    - Kill - "there is no there there".


__Interview: David Bland on Creating a Culture of Experimentation: Part 1__   
3:08 - 3:13
-_What does a workplace look like that is also practicing Lean Startup?_
- Lean Startup Workplace has: 
  -	Visual Management - use wall space if co-located, digitally (with teleconferencing and teamware) if not. 
  -	Customer interaction - split up in pairs and interview, bring customers into your digital/physical team work space.


__Interview: David Bland on Creating a Culture of Experimentation: Part 2__   
2:15 - 2:42
- _So, let's say we have a product that fundamentally has some kind of product market fit, people generally like it. And now we're remaining diligent and, feature by feature, we're releasing things. We're seeing what matters, what doesn't. And let's say we got two features out there. We think we've done reasonable testing to validate, they're a good idea. How do we stay focused? What should this team be watching for after the fact, to make sure that they're they're really right about that?_
- Instrument your product with analytics: Generate evidence as to what's actually going on with the product and launched features. The worst case is you're adding all these features, and it doesn't move the needle in any meaningful way. Nobody uses it more, or maybe even worse, they use it less because you added more features in. Use the pirate metric system of "acquisition, activation, retention, referral, and revenue" from Dave McClure.


__Resource:__ Slideshare deck from David McClure on 'Pirate Metrics'. While we use the AIDAOR framework to talk about the customer sales/relationship funnel, this is another popular alternative you might want to explore. 


__Interview: David Bland on Marrying Agile to Lean Startup__    
0:08 - 0:30
- _Let's talk about the actual work of integrating Lean Startup and Agile. Let's talk specifically about what's the difference between the user stories that are traditionally the focal point of what we do in Agile, and the hypotheses that are traditionally the focal point of what we're doing Lean Startup. How do you pair those together in a practical way?_


__Interview: Laura Klein on the Right Kind of Research__    
2:02 - 2:07
- _So I called these the problem, solution and implementation assumptions._
- The problem, solution and implementation assumptions: 
- Lean Startup requires asking: "How can I actually figure out if my value proposition is a good idea?" Lean experiments outline the basic assumptions that fall into three categories of: 
  -	Problem: Who are my users and what problem do they have? 
  -	Solution: Is this the right way to solve the problem? 
  -	Implementation: Am I capable of making the solution?



## Week 3 - How do we consistently deliver great usability?

"_The best products are tested for usability early and often, avoiding the destructive stress and uncertainty of a "big unveil." In this module, you’ll learn how to diagnose, design and execute phase-appropriate user testing. The tools you’ll learn to use here (a test plan template, prototyping tool, and test session infrastructure) are accessible/teachable to anyone on your team. And that’s a very good thing -- often products are released with poor usability because there "wasn’t enough time" to test it. With these techniques, you’ll be able to test early and often, reinforcing your culture of experimentation._

### Week 3: Identifying & Structuring a Usability Hypothesis

__The Always Test__   
2:20 - 2:48
- _All that said, what we're going to look at here is, just like we learned with the Lean Startup, the demand hypothesis material. How to isolate motivation and specifically to test for it. We're going to look at how you isolate usability and specifically tests for it. Why? Because if we try to test for these things at the same time, we're going to be using a method, that's probably wrong for one or the other of them. We're not going to get a little bit of both. We're just going to get a bunch of indeterminate junk, that's really just wasteful._
- To test usability, we start with user stories. They should be the focal point of conversations and as such, they are available to everyone to edit and change.


__The Always Test__   
3:41 - 3:44
- _This is an example of such a user story._
- User stories often have this format: "As a persona, I want to do something so that I can realize a reward." By "do something", we are referring to the software and what it does. "Reward" does not mean a winning the lottery kind of reward. IT can be something as simple as the user enters something into the program, and the software gives him feedback as to whether they entered the information correctly.


__The Inexact Science of Interface Design__   
1:02 - 1:13
- _Design, we've job to do. We have some measurable goal that we're designing for. That's really a definition of design is to have some intent towards some goal,_
- You achieve good design through: 
  -	Focus, 
  -	Consistency, 
  -	Experimentation 
- Focus. Good design is experienced as being relatively simple -- it's focused on what you need at that particular time. You have validated learning about who your user is, what they're trying to do, and why your proposition is better enough than their current alternatives to be actionable and investable. 
- Consistency. You should always use the same colors, typefaces, and other things from a style guide. Consistency is achieved through examined user patterns and tendencies, and translating that into what the user will be expecting to see here. 
 - __Resource:__ Style guide reference  <bit.ly/3tostyleguide>


__Diagnosing Usability with Donald Norman's 7 Steps Model__   
0:03 - 0:16
- _There is a really accessible set of ideas from Donald Norman's book the Design of Everyday Things that we're going to apply to achieving the scientific test oriented approach to achieving usability_
- The signifier is what the user understands a thing to do. 
- The affordance conveys what a given design will actually do when used. We want to make those Signifiers and affordances the same, and we have to test to validate the hypothesis, that the users are understanding this thing we're presenting to them consistent to the way that we expect. 
- The constraint is the limited ways that something (e.g. a software field for user input) can be used or manipulated. 
- Feedback: the user observing what is going on, and specifically understanding the signifier and affordance and what it does for him. 
- Mapping: the relationship between signifiers and affordances.


__Diagnosing Usability with Donald Norman's 7 Steps Model__   
3:18 - 3:45
- _The way that we kind of unpack these experiences is with these seven steps. And the classic example Donald Norman uses in his book is that he is sitting in an armchair, it's late afternoon, the light level is dropping and he can't see his book well enough, but he wants to keep reading. And so his goal is to keep reading, and really goal is equivalent to the term that we've been using problem scenario or job to be done._
- Donald Norman's 7 Steps:    
Identifying a goal (or, Job to be Done / problem Scenario)     
Feed-forward steps- what the user is going to do to achieve their goal: 
1. Plan: What are my alternatives? 
2. Specify: What can I do? Signifiers help the user to avoid unnecessary confusion as they're specifying these steps. Signifiers help to initiate the right action mapping to the appropriate affordances. 
3. Perform: How do I do it? Constraints help avoid error. 
Feedback steps - what the software tells the user after they perform the feed-forward steps: 
4. Perceive: What happened (after I performed the action(s))? Prompt, actionable feedback is critical. 
5. Interpret: What does it mean? A clear mapping and well understood model are necessary. 
6. Compare: Is this OK (have I achieved my goal)?


__Fixing Usability with Donald Norman's 7 Steps Model__   
0:01 - 0:10
- _We're going to spend a few minutes looking at the seven steps model and how the different layers relate to the jobs you have building software._
- Norman's 7 steps can also be sliced in layers: 
  - Reflective: 1. Plan & 6. Compare 
  - Behavioral: 2. Specify & 5. Interpret 
  - Visceral: 3. Perform & 4. Perceive


__Applying the 7 Steps Model to Hypothesis-Driven Development__   
0:50 - 0:54
- _Let's look at how they would unpack into the seven steps model._


__Applying the 7 Steps Model to Hypothesis-Driven Development__   
0:15 - 0:31
- _Now, the question is, how do we declare a usability hypothesis and then test it? And the best way to declare these usability hypotheses is just through writing good agile user stories_
-	Plan: In the plan step, the user is choosing an alternative. When we user test, however, we're going to supply motivation - i.e., we control for motivation (selecting our software) and only test the usability of the software. 
-	Specify: How natural is the mapping between the ideas our subject is carrying into the test to what we're showing them. Is it obvious to our subject how they would accomplish the tasks? 
-	Perceive and Interpret steps: What does the user think they are seeing, and what does that mean?


__Fixing the Visceral Layer__   
0:00 - 0:06
- _Let's talk a little bit about how to address problems with this visceral layer, if you think you have those._
- Recall: Normans 7 steps can also be sliced in layers:
  -	Reflective: 1. Plan & 6. Compare 
  -	Behavioral: 2. Specify & 5. Interpret 
  -	Visceral: 3. Perform & 4. Perceive 
- Use your company's style guide. If there isn't a style guide, use the company website to create a style guide. This way users will see something familiar if they switch from the company website to your application. 
- __Resource__: <bit.ly/25min-style>. This will help you create a style guide in 20 minutes. Above all, stay consistent with whatever else is going on with your company or your product, that's the easiest way to get a win here.


__Fixing the Behavioral Layer: The Importance of Comparables & Prototyping__    
0:01 - 0:05
- _Let's dive in and look at how you fix the behavioral layer._
- Recall: Normans 7 steps can also be sliced in layers: R
  -	Reflective: 1. Plan & 6. Compare
  -	Behavioral: 2. Specify & 5. Interpret 
  -	Visceral: 3. Perform & 4. Perceive


__Fixing the Behavioral Layer: The Importance of Comparables & Prototyping__    
0:51 - 1:04
- _In design and innovation you've gotta be ready to discard stuff all the time, and you gotta pair that with working in small batches and doing low fidelity things and building up to high fidelity things so you don't generate excessive waste._


__Fixing the Behavioral Layer: The Importance of Comparables & Prototyping__    
2:18 - 2:27
- _And there are some libraries here, pattern libraries, and we'll have those in the course resources. The whole point is you don't want to reinvent the wheel._
- What might users be expecting to find in this particular experience we're going to give to them? And how do we go out and look at how that's been approached elsewhere? Look at existing websites - not necessarily direct competitors, but other sites where the user may be taking a similar set of actions.
__Resource__: Pattern websites: 
  -	<ui-patterns.com> patternry.com/patterns web-patterns.net patterntap.com mobile
  -	<patterns.com> smiley cat.com/design_elements


__Fixing the Behavioral Layer: The Importance of Comparables & Prototyping__    
6:46 - 7:05
- _And say, for the same set of user stories, let's think about two really different ways that we might approach this. And if you can, you should bring both of those into testing. But let's just take a look at how they unpack their user stories and pair those with distinctly different concepts that deliver this user experience_
- Prepare a couple of prototypes that take different approaches to addressing the user story, and be prepared to discard one or both.


### Week 3 - How do you test Usability?

__Usability Testing: Fun & Affordable__   
0:09 - 1:04
- _Well, first and foremost, we need to make sure that we've rendered it into a nice, strong user story with a high-quality, third clause. That's the most important thing and one of the most critical habits for you to develop if you want to use hypothesis-driven development on the regular habitual basis that makes it really effective. From there, we need to make sure that we're isolating usability, and we're only testing it, we're not testing motivation at the same time, and that's important because we looked at how to use techniques from Lean Startup to structure and test a demand or a value hypothesis, and now we're going to use some very different methods to test usability. If you try to co-mingle these, what you're going to get is an inconclusive test that at best is unusable and at worst gives you some false conclusions about what's going on._
- How to test a usability hypothesis: 
  -	Make sure that you've created a strong user story with a high-quality, third clause. That's the most important thing and one of the most critical habits for you to develop if you want to use hypothesis-driven development on a regular habitual basis that is really effective. 
  -	Make sure that we're isolating usability, and we're only testing it, do not test motivation at the same time. That's important because we looked at how to use techniques from Lean Startup to structure and test a demand or a value hypothesis, and now we're going to use some very different methods to test usability. If you try to co-mingle these, the resulting test will give you results that at best are unusable, and at worst will provide you with false conclusions about what you are observing. Don't test motivation and usability at the same time.


__The Right Testing at the Right Time__   
1:32 - 2:12
- _Exploratory testing, we're investigating approaches, big different interface patterns, maybe likely with parallel prototypes. Assessment testing, we found one of those and we're just detailing and out looking at a few of the different cases and patterns if we're going to do this. Validation testing, we think we're fine and we're just validating that the software is really okay, and here we might time the user, pick up some metrics that we can look at and compare with our field metrics. All right. So the key thing is it's okay to be relatively informal, use rough prototypes, and if you can write a good user story, you can do this exploratory and assessment testing._
- Test the usability hypothesis by beginning early and moving through these different test phases: 
  -	Exploratory (approach will fundamentally work): We're investigating approaches using parallel prototypes that have big and contrasting interface patterns. 
  -	Assessment (the implementation is sound and ready for tuning): We selected one of the approaches from the exploratory phase and are getting into the detail of the interface and pattern with a few different use cases 
  -	Validation (ready for prime time): we're validating that the software is okay and we're creating some baseline metrics (e.g. based on user timing observation) that we can extend into field testing.


__A Test Plan Anyone Can Use__    
0:07 - 0:18
- _There's this test plan here, this is roughly what it looks like, and there is a reference in the course resources to both a template and a tutorial that deals with all the details._
- During exploratory and assessment testing, revise your your test plan, user stories, and prototypes as you go. Set aside time between test sessions for revisions. 
- Sections in the test plan template: 
  - Objectives and methods: clarifies the phase of testing (e.g., exploratory, validation) 
  - Product version: whether live or prototype with links to the software. 
  - Subjects: users who participated in test who passed the persona screener question(s). 
  - Research composition: agenda for testing - especially important if you are testing multiple things. 
  - Pre-session checklist: Dry run the test plan and check that the prototype is working as planned. 
  - Session design: Do not lead or "cue" the user. Ask them to perform tasks as to whether signifiers make sense and are intuitive. As you iteratively conduct tests, revise your test plan, user stories, and prototypes based on what you learn.
__Resource__: Usability test plan: <https://www.alexandercowan.com/usability-test-plan/>


__Creating Good Test Items__    
0:00 - 0:17
- _We're going to really dial in some detail at the heart of this test plan. Let's really analyze one of these specific examples, so you can think through how you're going to create your individual test items, and relate that to the seven steps model and really disciplined approach to assessing usability._
- Mapping Norman's 7 Steps to Usability testing: 
  -	Plan: Testers supply the motivation for the subjects and tell the users what they want to do (e.g., perform search) 
  -	Specify: Testers ask subjects how they would do an action when presented with a prototype (could be a paper cheater). Do the prototype signifiers direct them to their goal? yes or no. 
  -	Perform: Were the users able to achieve this and did the interface work as intended? 
  -	Perceive: Testers give or the prototype directs the users to the response page. Is it clear to the users that a response has been presented? 
  -	Interpret: Do subjects understand the results that have been presented to them?


__Running a Usability Design Sprint__   
0:00 - 0:05
- _How might you use a design sprint to test usability?_
- Assuming a five day design sprint: 
  - Inputs prepared ahead of sprint: a usability hypothesis in the form of rough user stories, a clear view of what you want to have happen (sprint agenda and checklist), test subjects, a clear view of the outputs (the test results) and what you're going to do with those. 
  - Day one: iterate on the user stories and develop wireframe/prototypes 
  -	Days two, three, four: perform tests, review immediate results, and iterate between tests 
  -	Day five: review test results and what you learned, what you will do next (develop, test), determine metrics to be used in further testing and in the field.


__Interview: Laura Klein on Qualitative vs. Quantitative Research__   
0:06 - 0:21
- _It's a lot of work to get started and figure out what you should do when. Can you talk a little bit about some of the distinctions you make between triggers that tell you right now, you need qualitative research versus right now, you need quantitative research?_
- Quantitative metrics, numbers, analytics tell you what is going on with your product. If you want to learn about your product and what is going on with it and how people are using it, what they're doing with it, use quantitative research. Use quantitative research to understand whether a product change resulted in the behavior change that the hypothesis predicted. 
- Qualitative research is talking to people and observing their behavior. It tells you why those things are happening. If you want to understand your user, their context, why they're doing the things they're doing, why they're making the choices they're making, use qualitative research. Use qualitative research to generate hypotheses.



## Week 4: How do We Invest to Move Fast?

"_You’ve learned how to test ideas and usability to reduce the amount of software your team needs to build and to focus its execution. Now you’re going to learn how high-functioning teams approach testing of the software itself. The practice of continuous delivery and the closely related Devops movement are changing the way we build and release software. It wasn’t that long ago where 2-3 releases a year was considered standard. Now, Amazon, for example, releases code every 11.6 seconds. This week, we’ll look at the delivery pipeline and step through what successful practitioners do at each stage and how you can diagnose and apply the practices that will improve your implementation of agile._"

### Week 4 - Identifying & Structuring a Functional Hypothesis

__The Team that Releases Together__   
0:00 - 0:17
- _As we get into this week, there are two concepts I want to introduce that are really central to this practice of testing your functional hypotheses as you go. One is a continuous delivery pipeline and the other is this practice of DevOps, which is a Domain Specific extension of Agile._
- Continuous Delivery Pipeline:   
Input: New Code 
1. Commit & Small (Unit) Tests 
2. Medium (Integration) Tests 
3. Large (System) Tests 
4. Manual Validation 
5. Deployment 
Output: Released Product


__The Team that Releases Together__   
1:30 - 1:58
- _The other concept is DevOps. Whereas Agile is this thing that deals with the business people, the product managers, and the development team as a general entity. DevOps is a Domain Specific extension of Agile that deals with the relationship between those two folks, but also in particular the relationship between developers, testers, and operations or systems folks. Let's look at what that means in a little bit more detail._
- _In the old world, we had silos consisting of Development, Test and Operations. The inputs to the developer were user stories, wireframes, or, in the very old world, requirements. The Development output is software that Dev says is ready to be deployed. They pass this over to the tester. Test provides working software, some documentation notes on how it's supposed to work. The Test output is given to the ops people. Ops packages and releases the software and provides support for it._ 
- DevOps restructures the relationship between DEV, Test and Ops' teams so they are more integrated in their work. 
-	Some DevOps Themes: Test Driven Development, Automated Testing, Automation Scripts for software upgrades.


__Getting Started with Continuous Delivery__    
0:00 - 0:05
- _Now, how do you get started with continuous delivery? It could be daunting._
- Gather DEV, Ops, and Test teams along with the product owner, and other members of the Agile team. 
- With full team, diagram your process. Sometimes it is easier to work backwards starting with the product release and documenting the antecedents of each step. 
- Once you have a documented process, develop a hit list of process improvements - e.g., manual processes in testing and release. 
-	Consider continuous integration / Continuous delivery platform, like Jenkins, to centralize and automate releases.


__Anders Wallgren on Getting Started__    
0:11 - 0:23
- _Let's talk about getting started with continuous delivery. What is literally is the very first thing you would recommend an organization does, if they want to invest in this generally?_
-	Document the end to end process from code commit to end user software delivery. 
-	Document manual steps, automated steps, approvals, and where the process is simple, and where it is complex. 
-	Document process pain points where errors frequently occur or time to execute is long, and look here first for process improvements.


__Anders Wallgren on Getting Started__    
1:34 - 1:45
- _How do you figure out where to start investing time, infrastructure changes, process improvements, whatever is going to work?_


### Week 4 - What kind of tests can I run?

__The Test Pyramid__    
0:10 - 0:27
- _Let's talk a little bit about the type of tests and how many of you want and why. Well, generally, the test quantities are framed in this pyramid, or I guess ziggurats, and these are steps in this case. So you have a lot of these unit or small tests that test an individual piece of code._
- Test Pyramid - Large amount of unit tests. Lesser amount of integration tests, and even fewer system tests. 
- Unit Tests - Test individual pieces of code e.g., a function or method that strips the domain from a user email. 
    - Cost - least expensive. 
    - Speed - fast to develop and implement. 
    - Isolation - high degree of isolation because if the test fails then it is the fault of the individual code. 
    - Usually created by the developer as a necessity.
- Integration test - Tests the specific interaction between two components of a system. 
   - Cost - more than unit. 
   - Speed - slower than unit. 
   - Isolation - less than unit. 
   - Less likely to find someone to write an integration test. 
- System Test - Tests the entire system from the user perspective. 
   - Usually developed by dedicated test resource using a software test package. Prone to false results that can swamp developers. 
   - Cost - most. 
   - Isolation - least (could be a false result, and if valid then hard to isolate. 
   - Speed - slowest.


__The Commit & Small Tests Stage__    
0:00 - 0:07
- _Let's talk about the small tests and commit phase here in a little bit more detail._
- Commit phase - New code comes out of version control and is committed, and built in whatever way it needs to be to be executed, and then unit tests are run. 
-	The code goes into an artifact repository so that the build will be available to the downstream tests. 
-	Unit tests don't touch anything and run in memory separately. 
-	When developers write unit tests, they create test doubles (AKA dummy, stub, spy, mocks) to give their code something to interact with in place of users. Doubles simulate the input and interactions that these functions would normally have with users. 
-	Test-driven development is the default position for a lot of modern development and most teams, but there is a lot of work involved in creating doubles and writing unit tests. There is some debate about whether it's really worthwhile to perform unit testing under certain circumstances.


__The Commit & Small Tests Stage__      
0:35 - 0:42
- _So they really don't touch anything, they run in memory as their own little atomic thing and that may seem a little bit weird.
See slide._


__The Job of Version Control__    
0:00 - 0:04
- _Let's talk about Version Control a little bit._
-	Version Control - Agreed upon process and policy to ensure that all team members can access a recent working copy of software (or document) that receives and incorporates everybody's changes while allowing individuals to freely make changes on their own without breaking the software. 
-	Trunk - the main version of the software that is periodically updated. 
-	Branch - The work/additions/changes of individual contributors. Team members will want to make changes by taking their own copy of the trunk, and making modifications to it. 
-	Merge - Committing branch modification back to the main version of software (or document). The footprint of unmerged code should be minimized in continuous delivery. 
-	Teams may manage their trunks and branches differently, but it is crucial that everyone on the team understands the version control policy.


__Medium Tests__    
0:01 - 0:06
- _Let's talk a little bit more about this medium test age here._
- Integration tests include other system components, like the operating system and other adjacent pieces of code and subsystems, but not external systems and not the network.


__Medium Tests__    
0:25 - 0:30
- _it seems like if we're not used to just a small difference between here to here, why don't we just sort of run them as all one thing?_
- Three main reasons that integration testing is separate from system testing: 
1. Speed: for the developer, it's important integration tests run quickly, rather than executing full system tests which would require more of their time. 
2. Problem isolation: if a piece of code passes the unit test and then fails the integration test, this will help isolate the problem. It will be isolated to an interaction with, for example, the file system, or the database. This will be helpful for tracking on a problem and getting it fixed in minimum time. 
3. Organization: integration testing helps to organize software updates to have some meaningful distinctions for housekeeping purposes. It creates rules that if a given part of the system changes, then it is agreed that these people responsible for other system components should help with updating this part of the test regime.


__Large Tests__   
3:02 - 3:14
- _Let's talk about this large test type. So this is again, this big catch-all for everything, and it's okay that these run a little longer, we don't want them to be any longer than they have to be._
- You may still need a stub service to simulate the interaction with some external systems, especially if those external systems are hard to access, do not welcome test transactions, slow, unpredictable, under-development themselves, or not well-documented


__Large Tests__   
0:00 - 0:05
- _Let's talk about these large or system tests in a little more detail._
- System Testing includes the following test types/facets: 
  -	Functional tests: testing how the overall system works with less focus on underlying components. 
  -	Non-functional tests: Performance, capacity, and security testing. These are usually run in a separate environment that looks like production, and is well-controlled. 
  -	Acceptance test - validate that the software conforms to the user stories in a way that works for the systems team and is acceptable to release the customers.


__Anders Wallgren on Functional Testing__   
0:14 - 0:23
- _Before we do that, let's talk about behavior-driven development a little bit. Can you give us your layman's view of what it is and why it might be important?_
- Behavior-driven development - main goal is to allow non-programmers to specify the behaviors of the system, in a way that can then be automatically tested. It is part of the kind of test-first culture in some. It allows you to specify very ornate behaviors of the system and requires project managers and product managers to think in terms of, "how do I express this for testability?"


__Release Stage__   
0:01 - 0:05
- _Let's talk about this release or deploy stage._
- Release stage: Builds are coming from the artifact repository to ensure that it matches what passed through the prior validation phases, and the configuration is drawn from version control. 
  =	Using configurations from version control facilitates the practice of continuous delivery by allowing for standardization and automation. 
  -	The builds from the artifact repository (presumably) have been through all of the testing steps and should be used during release to for more predictable behavior. 
- Take time to manage changes in a way that is consistent with the continuous delivery process. Production issues often arise when configurations are changed manually during production. Teams should go back to make sure they have a systematic understanding of problems and try to stick to systematic changes consistent with the overall continuous delivery pipeline.


__Release Stage__   
1:12 - 1:26
- _it's really important that both the process in the environments that you're using look like the same environments you were using throughout the rest of the pipeline here._
- Its less important to replicate hardware between the production and test environments, and more important that the application configuration and digital layer match between environments.


__Release Stage__   
1:59 - 2:05
- _Another really important thing, early capability to focus on is the ability to roll back for obvious reasons._
-	Canary releasing - a process to deploy new software at first to a subset of users and you validate it's working okay. 
-	Blue green release - pattern where you have two parallel environments running, the blue and the green and routing controls which is production. For example, if blue is active, you deploy to the green environment and you make sure everything is okay in real life, with blue as your rollback. And then you change the routing to go to green, making it the live prod environment instead of blue. Blue is where you will deploy the next release.


__The Job of Deploying__    
4:25 - 4:46
- _Kubernetes can do stuff like, one data center goes down that had five of these little pick another good choice of a datacenter and automatically spin those up, rather than panic and calls and overhead and who can install that? And do they have access to that datacenter? And where do we put these and all those terrible things that used to happen before we have Container Orchestration._
-	Container Orchestration - tools, like Kubernetes, that can automatically create or bring down environments based on events. For example, if an app needs 10 instances in PAC region, and a data center goes down, Kubernetes can find an appropriate data center and create the needed instances in it. 
-	Feature flags - allow you to place new code into a system, but "flag" it off in the configuration. You can turn it on for a subset of users, and roll back or more widely release if it is stable and is having the desired effect on user behavior.


__The Job of Deploying__
6:37 - 6:54
- _So that is how the teams are using new tools, new practices, and new ways of working together to do well on the Java conduce delivery and increase the frequency of their releases.__    
Infrastructure as Code tools: 
-	Configuration management tools - automate and standardize the upgrade process or installing new instances, e.g., Puppet and Chef, instead of writing out a list of instructions that a human has to read and follow. Using tools like Chef, developers, testers, and operations will collaborate on an upgrade and install code. 
-	Containers - Containers, like Docker, abstract away an individual unit of your application from the underlying physical host and operating system so that it can run on a developer's laptop, a test system, or a production system in essentially the same way. A container decouples an application and its behavior from the underlying hardware and operating system. So, all the elements/stuff that's going to have the variation is placed into this container so that that's more consistent across those different environments.


__Anders Wallgren on Deployment__   
1:32 - 1:43
- _Yeah, I mean, I think the same kind of reason why continuous integration is very valuable to teams' continuous delivery in all the disciplines that you have to build around it can be very valuable too._
- Continuous Delivery gives you very rapid feedback on the outcome and quality of the work that you just did and so you get it when the work is fresh in your mind, not 90 days later. Also, the longer you go between when problem is introduced to when you discover it, the more expensive it is to mitigate. Finally, continuous delivery makes teams look at how bugs can be discovered earlier in the test and release process.


__Chris Kent on Continuous Deployment__   
3:53 - 5:14
- _We're using a tool called Cucumber, and that is a tool that allows you to write your acceptance tests in plain language, and then have a developer and sometimes partner with a tester to do translate that plain language into executable code effectively. The beauty behind this, in ideal world, what you have is, you have three people sitting down together to define your acceptance criteria. You've got the business stakeholder, and ideally they're the ones writing that plain language, you've got a tester and you got a developer. Those three people hash it out and they figure out, okay, here's the plain language that describes what we're going to do, here are the scenarios that describe the inputs and outputs for this feature that we're working on. Given that the developer and sometimes the tester can go off and actually implement those scenarios. But what do you end up with is a suite of tests that you can hand to non-technical folks, you can hand it to your manager and they can look as a, okay, this is what our application does and it's what you end up with is considered to be a living documentation system._


__Chris Kent on Continuous Deployment__   
6:24 - 7:38
- _we would sit down right there at the beginning of the sprint and say, all right, which of these stories has user facing impact? Which ones would they actually notice a change? For any of those stories that had a noticeable change, we gave it what we call The Scarlet Letter, so with big red marker, and we wrote an A on the ticket with a big circle around it and that to us meant it had to go through our acceptance test-driven development process, which meant that we would have that meeting with the test engineer, developer and whoever we could drag in to be a business stakeholder to define out those tests and from there, one or two things would happen, either the test would be written before the development efforts started, that's the ideal, or at the very least they'd be written in parallel with the development effort. So now not only do we have documentation in the form of tests, we also have a definition of done for the work that's being done, the coding that the developer has to do, as soon as that is acceptance tests passed, assuming you've done your due diligence in defining what scenarios are, as soon as that passes, your work is done, your feature is complete._


### Week 4 - How do you create a working environment that's friendly for continuous delivery?

__Test-Driven General Management__    
0:01 - 0:18
- _Let's talk about what it means to be test driven in our general practices, the general management of our team. I would say some of the highlights are that you're consistently holding retrospectives where you think about how things are going, what you want to change, what you want to keep the same._
- A 1-2 hour retrospective for a two-week sprint, is approximately appropriate. 
- Create a simple agile team charter that provides basic explanations of your agile practice implementation. See a template and some examples in the lesson resource material. 
- The charter will contain: 
  -	Problem focus: personas & problem scenarios 
  -	Team roles - e.g., the product owner, scrum master, Agile coach, and other roles you've delineated. 
  -	Method parameters - e.g., retrospectives, 1 week or 2 week sprints. 
  -	Team tools (e.g. Trello, agile central) 
  -	Conventions and practices - Key processes used and why - whether for Agile or for software development. E.g., code reviews, tests. 
  -	Log - key decisions (on how to get better)


__Test-Driven General Management__    
3:06 - 3:25
- _another really critical part of probably the most important thing in beyond the sort of basic practices we've talked about is to hold these retrospectives. It's hard to be test driven if you don't look at results and really think about them and talk about them with your team in a substantial way_
- Retrospective: 
  -	Should be one hour (minimum) for a 2-week sprint. 
  -	Scrum Master/Agile Coach should prepare an agenda: how does team feel the project is going, what is on their a-list, what are the hardest things to do. 
  -	If you are running experiments in Lean Start-up, it is critical to look at experiment results and ask why


__Narrative and the 'Happy Path'__    
0:00 - 0:04
- _Let's talk a little bit about how you focus your testing.
Do not try to test every conceivable thing that all the available functions in the target area could do - you will probably not complete that testing. Instead focus on the happy path._
-	Exploratory testing - explore, learn and test based on input from the product owner, the "Story Map", or from observing users. 
-	A critical contrast between agile and legacy methods is the difference between trying to arbitrarily test every single thing that the system can do, and iteratively working towards understanding the happy paths that users will take through the application.


__Narrative and the 'Happy Path'__    
2:24 - 2:42
- _There's this idea of a use case and quite a lot in the last year, I've heard people use this in a casual way just to mean that, I think, something that the user might do, which is okay. Words are words. There is actually a clinical definition of this use case, and this has the principal elements of it._   
Use case includes: 
-	Title: Primary Actor 
-	Goal in Context 
-	Scope 
-	Stakeholders and Interests 
-	Precondition 
-	Minimal Guarantees 
-	Success Guarantees 
-	Trigger 
-	Main Success Scenario 
-	Extensions 
-	Technology and Data Variations List


__The Emergence of DevOps and the Ascent of Continuous Delivery__   
1:09 - 1:20
- _in devops, we have an overall movement towards converging these things in terms of roles, so people doing multiple parts of this at the same time._    
Developers and testers work together to explore: 
-	What is the narrative that we're getting into the system? 
-	What are the things that we're learning about? 
-	What's really valuable and what are users actually doing with this software that we're producing? 
-	How do we sharpen our test cases and make them more relevant to this? 
Developers and Testers are very focused on encapsulating all this learning into automated test suites, so that they can improve their practice of continuous delivery.


__The Emergence of DevOps and the Ascent of Continuous Delivery__   
2:17 - 2:18
- _What about operations?_    
Ops has to keep the infrastructure secure, and scalable, and up and running, and figure out how to do that better and better. In a devops environment, ops is actively collaborating with the development and the test teams to: 
-	Operationalize the software, based on what's changing and what they're learning. 
-	Build self-service infrastructure using tools like Puppet and Chef and participating in the continuous delivery pipeline that we talked about.


__Design for Deployability__    
0:00 - 0:23
- _Let's talk a little bit about the relationship between product design and deployability, the ability to continuously deploy our software successfully. One of the things that can really interfere with that process is having a lot of different configuration parameters, especially configuration parameters that affect things system-wide._
- Different configuration parameters, especially those that affect things system-wide, create drag and interfere with continuous delivery. Products will get into a situation where they don't know if they should go left or go right. So they make it configurable. Good decision or a bad decision, this can really create a huge drag on your ability to successfully test the system variation in a meaningful way. So one of the things that you should always be thinking about is making sure that you really understand what's valuable to your user.


__Anders Wallgren on Continuous Deployment__    
0:21 - 0:31
- _What does the team or the individual that's trying to change the way that their team works, what do they get from a successful implementation of a continuous delivery capability?_
- Continuous delivery focuses the team on reducing cycle time. The team nails down the process from commit to delivery, and notes what is done by people and what is done by computers. Continuous delivery teams are aware of where value is created in the process, and looks for improvements there. Where value is not being created, those steps are eliminated, made optional, or ran in parallel to something else. Automation is leveraged to enable process optimization.


__Using "Day in the Life" to Drive Empathy__    
0:16 - 0:36
- _Now, I mentioned I would talk more about Day in the Life, so I'm going to show you an example of that. Day in the Life is a game you play with a bunch of collaborators to help bring a persona to life. And the way it works is I'm going to show you a series of photos about a persona and now I'm going to ask you a bunch of questions about them._
- "Day in the Life" enables teams to humanize and get to know the user persona. This knowledge helps teams make better decisions and deliver a more valuable product.


